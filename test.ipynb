{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Welcome Ahmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() #Load the enviroment variables from the .env file\n",
    "api_key = os.getenv(\"Langchain_project_openai\")\n",
    "print(api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.hub import TRANSFORMERS_CACHE\n",
    "\n",
    "print(TRANSFORMERS_CACHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aelkhodary/Documents/GitHub/Books/DATA/LLM/openai-generative-ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-27 16:56:20.334994: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Downloading shards:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"How many r in strawberry?\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "tokenized_message = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True)\n",
    "response_token_ids = model.generate(tokenized_message['input_ids'].cuda(),attention_mask=tokenized_message['attention_mask'].cuda(),  max_new_tokens=4096, pad_token_id = tokenizer.eos_token_id)\n",
    "generated_tokens =response_token_ids[:, len(tokenized_message['input_ids'][0]):]\n",
    "generated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "print(generated_text)\n",
    "\n",
    "# See response at top of model card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9m/qp7n28sd3d13_ywgjkkwnbb00000gn/T/ipykernel_51829/1961412186.py:18: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  model = HuggingFaceHub(\n",
      "/Users/aelkhodary/Documents/GitHub/Books/DATA/LLM/openai-generative-ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response one: Tell me a sad joke about British Weather. Or at least I think it should be. I think the first thing that has to do with weather is to get the weather of the country right. And when you create it, you have to do it with objective facts.\n",
      "\n",
      "So it's a fact that when you do the weather, you see that when there's a big storm, you see that, then there's a big storm. But what really goes on here is that the weather has to be determined by the weather system, and\n",
      "Response two: Tell me a funny joke about cats. I'll let you in.\n",
      "\n",
      "It's actually a hilarious thing to do when you're in a situation where you're doing some sort of job. To be honest, one thing I found interesting about being a troll in a moment of real need was that I had some real friends. I just wanted to find out if they were a troll, and if they were not.\n",
      "\n",
      "So I ran into a couple of people that were on Twitter, and they had a buddy that was a big\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain import HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Create the Prompt Template\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "\n",
    "# Format the prompt using parameters\n",
    "formatted_prompt = prompt_template.format(adjective=\"sad\", content=\"British Weather\")\n",
    "\n",
    "# Create the Hugging Face Hub LLM Object\n",
    "model = HuggingFaceHub(\n",
    "    repo_id=\"gpt2\",  # You can replace 'gpt2' with another model of your choice\n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 50}\n",
    ")\n",
    "\n",
    "# Invoke the Model Directly\n",
    "response_one = model.invoke(formatted_prompt)\n",
    "print(f\"Response one: {response_one}\")\n",
    "\n",
    "# Create the Chain\n",
    "# Note: You must use the prompt template object to build the chain.\n",
    "chain = prompt_template | model\n",
    "\n",
    "# Run the Chain with specific input values\n",
    "response_two = chain.invoke({\"adjective\": \"funny\", \"content\": \"cats\"})\n",
    "print(f\"Response two: {response_two}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
