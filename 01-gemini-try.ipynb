{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Import Google API Key\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Set yout LLM to Google Gemini Model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro-latest\",  google_api_key=os.getenv(\"Langchain_project_google_gemini\"))\n",
    "\n",
    "response = llm.invoke(\"Who is Mohamed Hosny Moubark\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert your code to use the OpenAI API instead of Google’s Gemini model, you’ll need to adjust the imports and replace the ChatGoogleGenerativeAI class with the relevant class for OpenAI's models. Here's the modified code:\n",
    "\n",
    "Prerequisites\n",
    "Install required packages:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "pip install openai python-dotenv langchain\n",
    "Get your OpenAI API key:\n",
    "Visit OpenAI API to generate your API key if you don’t have one already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client with the API key\n",
    "client = OpenAI(api_key=os.getenv(\"Langchain_project_openai\"))\n",
    "\n",
    "# Create a chat completion request\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\", #\"gpt-3.5-turbo\",  # You can also use 'gpt-4'\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who is Mohamed Hosni Mubarak?\"}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Print the assistant's response\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How LangChain Integrates with Hugging Face Models\n",
    "Using Hugging Face Models as LLMs:\n",
    "\n",
    "LangChain allows you to directly utilize Hugging Face-hosted models (like GPT-2, BERT, or BLOOM) for text generation or completion tasks. This is particularly useful if you want to use open-source models instead of relying on proprietary ones like OpenAI's GPT-4.\n",
    "Example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# Initialize Hugging Face model using your API key\n",
    "llm = HuggingFaceHub(repo_id=\"gpt2\", model_kwargs={\"temperature\": 0.7})\n",
    "\n",
    "# Call the model with input text\n",
    "response = llm(\"Who is Mohamed Hosni Mubarak?\")\n",
    "print(response)\n",
    "Integration with Hugging Face Hub:\n",
    "\n",
    "Hugging Face's Hub provides access to many pretrained models. Using LangChain’s HuggingFaceHub integration, you can easily load these models for tasks like summarization, translation, and more by specifying the model's repo_id from the Hugging Face Hub.\n",
    "Hugging Face Pipelines with LangChain:\n",
    "\n",
    "LangChain also allows integration with Hugging Face's pipeline API, which provides predefined pipelines for tasks such as question answering (QA), text generation, and translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "# Initialize a Hugging Face pipeline for text generation\n",
    "#generator = pipeline(\"text-generation\", model=\"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\") # size 123G\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\") # size 600MB\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# Generate text using the pipeline\n",
    "response = llm(\"Explain the concept of love.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Yes, frameworks like LangChain simplify the interaction with large language models (LLMs) by handling many low-level processes, including tokenization, tensor management, and input formatting. Let's dive into how LangChain abstracts these complexities and what makes it beneficial.\n",
    "\n",
    "What LangChain Does:\n",
    "LangChain is a framework for building applications using LLMs, such as chatbots, question-answering systems, or summarization tools. One of its main strengths is abstracting backend operations, so you don’t have to manually:\n",
    "\n",
    "Tokenize inputs into input_ids and attention_mask.\n",
    "Convert inputs to tensors.\n",
    "Call low-level model APIs (like model.generate()).\n",
    "Handle batch preparation or memory optimization.\n",
    "Instead, LangChain allows you to pass a simple prompt as a text string, and it takes care of all backend processing required to communicate with the model.\n",
    "\n",
    "````\n",
    "## How LangChain Handles Backend Operations:\n",
    "```html\n",
    "How LangChain Handles Backend Operations:\n",
    "\n",
    "1-Tokenization and Tensor Conversion:\n",
    "\n",
    "When you provide a prompt, LangChain automatically tokenizes the input behind the scenes using the appropriate tokenizer (depending on the model you're using, such as GPT, BERT, or Llama models).\n",
    "It converts the tokens into tensors (if needed) and sends them to the model for inference.\n",
    "\n",
    "2-API Calls and Prompt Management:\n",
    "\n",
    "LangChain can manage API calls to cloud-hosted models like OpenAI’s GPT-3/GPT-4 or local Hugging Face models.\n",
    "It optimizes input handling by truncating or batching the prompts, ensuring you don’t exceed the model's token limits.\n",
    "\n",
    "3-Handling Complex Workflows:\n",
    "\n",
    "LangChain can chain multiple operations (e.g., summarization + question answering).\n",
    "It also integrates with memory management (for chatbots to remember previous inputs) and retrieval-augmented generation (RAG) workflows, where it retrieves external data to feed into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: How LangChain Simplifies LLM Usage\n",
    "Without LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Prepare the input prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate the response\n",
    "output = model.generate(input_ids, max_length=50)\n",
    "print(output)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LangChain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI  # Correct import for chat models\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure the API key is loaded properly\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(openai_api_key)\n",
    "\n",
    "\n",
    "# Initialize the model (using OpenAI’s GPT-3/4 or another LLM)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Provide the input prompt directly\n",
    "response = llm.predict(\"What is the capital of France?\")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To integrate LangChain with the Hugging Face Transformers library, you can use the HuggingFaceHub or HuggingFacePipeline class in LangChain. This setup allows you to work with models directly from Hugging Face’s library using LangChain’s interface.\n",
    "\n",
    "Below are two approaches:\n",
    "\n",
    "1-Using Hugging Face’s hosted models via API (requires a Hugging Face API key).\n",
    "2-Using local Hugging Face models with transformers library (without API key).\n",
    "\n",
    "Which Approach Should You Use?\n",
    "Approach 1: Use this if you want access to Hugging Face Hub models without setting up local models. Requires an API key.\n",
    "Approach 2: Use this if you want to run everything locally without relying on external APIs. Ideal for offline use or when you need more control over the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 1: Using Hugging Face Hub API with LangChain\n",
    "This approach assumes you have a Hugging Face API key to access their models hosted on the cloud.\n",
    "\n",
    "Explanation:\n",
    "Hugging Face Hub API: This code accesses a hosted GPT-2 model or any other available model on Hugging Face’s Hub.\n",
    "HuggingFaceHub Class: LangChain’s HuggingFaceHub class simplifies interactions with the Hugging Face API.\n",
    "Model Customization: Parameters like temperature and max_length are passed through model_kwargs.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n",
      "\n",
      "The capital of France is the city of Paris. It is about half an hour's drive from the capital of the European Union (EU).\n",
      "\n",
      "How long is the longest continuous continuous day in the year in France?\n",
      "\n",
      "The longest continuous continuous day in the year in France is the shortest day of the year. In the summer months, it is the middle of July.\n",
      "\n",
      "How many total days in a year do French people live in?\n",
      "\n",
      "The total number of days\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure the API key is loaded properly\n",
    "huggingface_api_key = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# Initialize the Hugging Face Hub model via LangChain\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"openai-community/gpt2-large\",  # Replace 'gpt2' with the desired model\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 100},\n",
    "    huggingfacehub_api_token=huggingface_api_key,\n",
    ")\n",
    "\n",
    "# Provide the input prompt directly\n",
    "response = llm(\"What is the capital of France?\")\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach 2: Using Local Models from transformers with LangChain\n",
    "If you want to avoid API usage and run models locally using the transformers library, follow this approach.\n",
    "\n",
    "Explanation:\n",
    "HuggingFacePipeline Class: This allows you to integrate Hugging Face models directly with LangChain using a local transformers pipeline.\n",
    "Local Execution: No need for an API key since the model runs locally.\n",
    "Pipeline API: Handles tokenization and model inference internally, simplifying the usage.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the model and tokenizer locally\n",
    "model_name = \"gpt2\"  # Or use any other model like 'EleutherAI/gpt-neo-2.7B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Create a text-generation pipeline using Hugging Face\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Initialize the LangChain model with the Hugging Face pipeline\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Provide the input prompt directly\n",
    "response = llm(\"What is the capital of France?\")\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
