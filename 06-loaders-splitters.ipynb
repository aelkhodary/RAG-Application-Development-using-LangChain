{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e95febf1",
   "metadata": {},
   "source": [
    "## This Notebook is to demonstrate commonly used Loaders and Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018429b7",
   "metadata": {},
   "source": [
    "#### In LangChain, a Document is a simple structure with two fields:\n",
    "- `page_content (string)`: This field contains the raw text of the document.\n",
    "- `metadata (dictionary)`: This field stores additional metadata about the text, such as the source URL, author, or any other relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50d1439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'loaders-samples/sample.txt'}, page_content='The Lorem ipum filling text is used by graphic designers, programmers and printers with the aim of occupying the spaces of a website, an advertising product or an editorial production whose final text is not yet ready.\\n\\nThis expedient serves to get an idea of the finished product that will soon be printed or disseminated via digital channels.\\n\\nIn order to have a result that is more in keeping with the final result, the graphic designers, designers or typographers report the Lorem ipsum text in respect of two fundamental aspects, namely readability and editorial requirements.\\n\\nThe choice of font and font size with which Lorem ipsum is reproduced answers to specific needs that go beyond the simple and simple filling of spaces dedicated to accepting real texts and allowing to have hands an advertising/publishing product, both web and paper, true to reality.\\n\\nIts nonsense allows the eye to focus only on the graphic layout objectively evaluating the stylistic choices of a project, so it is installed on many graphic programs on many software platforms of personal publishing and content management system. ')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    " \n",
    "# Load text data from a file using TextLoader\n",
    "loader = TextLoader(\"loaders-samples/sample.txt\")\n",
    "document = loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb6ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3298fbd",
   "metadata": {},
   "source": [
    "### Types of Document Loaders in LangChain\n",
    "\n",
    "#### LangChain offers three main types of Document Loaders:\n",
    "\n",
    "- `Transform Loaders`: These loaders handle different input formats and transform them into the Document format. For instance, consider a CSV file named \"data.csv\" with columns for \"name\" and \"age\". Using the CSVLoader, you can load the CSV data into Documents.\n",
    "- `Public Dataset or Service Loaders`: LangChain provides loaders for popular public sources, allowing quick retrieval and creation of Documents. For example, the WikipediaLoader can load content from Wikipedia.\n",
    "- `Proprietary Dataset or Service Loaders`: These loaders are designed to handle proprietary sources that may require additional authentication or setup. For instance, a loader could be created specifically for loading data from an internal database or an API with proprietary access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f1193",
   "metadata": {},
   "source": [
    "### Transform Loader example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSVLoader\n",
    "\n",
    "from langchain.document_loaders import CSVLoader\n",
    " \n",
    "# Load data from a CSV file using CSVLoader\n",
    "loader = CSVLoader(\"HR-Employee-Attrition.csv\")\n",
    "documents = loader.load()\n",
    " \n",
    "# Access the content and metadata of each document\n",
    "for document in documents:\n",
    "    content = document.page_content\n",
    "    metadata = document.metadata\n",
    " \n",
    "    # Process the content and metadata\n",
    "    print(content)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fed6dd",
   "metadata": {},
   "source": [
    "### PDFLoader\n",
    "Loads each page of the PDF as one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31cffb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"Software-Engineer-CV.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa996a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for page in pages:\n",
    "    cnt = cnt+1\n",
    "    print(\"---- Document #\", cnt)\n",
    "    print(page.page_content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190f50e",
   "metadata": {},
   "source": [
    "### WebBaseLoader\n",
    "This covers how to use WebBaseLoader to load all text from HTML webpages into a document format that we can use downstream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee139eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.ibm.com/us-en/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a2965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine strip() with string formatting for basic formatting\n",
    "formatted_text = data[0].page_content.strip().replace(\"\\n\\n\", \"\\n\")  # Replace double newlines with single newlines\n",
    "\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions for more comprehensive cleaning:\n",
    "import re\n",
    "\n",
    "# Remove unnecessary whitespace and multiple newlines\n",
    "cleaned_text = re.sub(r\"\\s+\", \" \", formatted_text)  # Replace multiple spaces with single space\n",
    "cleaned_text = re.sub(r\"\\n+\", \"\\n\\n\", cleaned_text)  # Limit newlines to two per paragraph\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08b6be",
   "metadata": {},
   "source": [
    "### JSON Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56be83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ed25591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "file_path='loaders-samples/sample.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f27fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dd61800",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader =JSONLoader(\n",
    "    file_path=\"loaders-samples/sample.json\", \n",
    "    jq_schema=\".employees[].email\", \n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c86a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42fc693",
   "metadata": {},
   "source": [
    "## Public Dataset or Service Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475554f7",
   "metadata": {},
   "source": [
    "### Wikipedia Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34b1625f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    " \n",
    "# Load content from Wikipedia using WikipediaLoader\n",
    "loader = WikipediaLoader(\"Machine_learning\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54bc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83503ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "document[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7832817",
   "metadata": {},
   "source": [
    "### IMDB Movie Script Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f48cdd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import IMSDbLoader\n",
    "\n",
    "loader = IMSDbLoader(\"https://imsdb.com/scripts/BlacKkKlansman.html\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b4e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary newlines and carriage returns\n",
    "formatted_text = data[0].page_content[:5000].strip()\n",
    "\n",
    "# Print the formatted text\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d07e7b",
   "metadata": {},
   "source": [
    "### YouTubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e0ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e775381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from youtube_transcript_api._errors import NoTranscriptFound\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=8grWlJSRg_w\", add_video_info=False\n",
    ")\n",
    "# Set the language to Arabic\n",
    "loader.language = ['ar']\n",
    "try:\n",
    "    data = loader.load()\n",
    "    print(data)\n",
    "except NoTranscriptFound:\n",
    "    print(\"No transcript available for this video in the specified language.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7956e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install youtube-transcript-api\n",
    "!pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from youtube_transcript_api._errors import NoTranscriptFound\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=8grWlJSRg_w\", add_video_info=False\n",
    ")\n",
    "# Set the language to Arabic\n",
    "loader.language = ['ar']\n",
    "try:\n",
    "    data = loader.load()\n",
    "    print(data[0].metadata)\n",
    "    print(data[0].page_content)\n",
    "except NoTranscriptFound:\n",
    "    print(\"No transcript available for this video in the specified language.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary newlines and carriage returns\n",
    "formatted_text = data[0].page_content[:5000].strip()\n",
    "\n",
    "# Print the formatted text\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957434c",
   "metadata": {},
   "source": [
    "#### Add Video preferences, Add language preferences\n",
    "- Language param : It’s a list of language codes in a descending priority, en by default.\n",
    "- translation param : It’s a translate preference, you can translate available transcript to your preferred language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935023f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from pytube.exceptions import PytubeError\n",
    "\n",
    "loader = YoutubeLoader.from_youtube_url(\n",
    "    \"https://www.youtube.com/watch?v=8grWlJSRg_w\",\n",
    "    add_video_info=False,\n",
    "    language=[\"ar\", \"id\"],\n",
    "    translation=\"en\",\n",
    ")\n",
    "\n",
    "try:\n",
    "    ytdata = loader.load()\n",
    "    print(ytdata)\n",
    "except KeyError:\n",
    "    print(\"The 'videoDetails' key is missing, possibly due to restricted access or changes in YouTube's metadata format.\")\n",
    "except PytubeError as e:\n",
    "    print(f\"An error occurred with Pytube: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc99420",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary newlines and carriage returns\n",
    "formatted_text = ytdata[0].page_content[:5000].strip()\n",
    "\n",
    "# Print the formatted text\n",
    "print(formatted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e761d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytube moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pytube import YouTube\n",
    "from moviepy.editor import AudioFileClip\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "video_url = \"https://www.youtube.com/watch?v=8grWlJSRg_w\"\n",
    "\n",
    "max_retries = 3\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        yt = YouTube(video_url)\n",
    "        video_stream = yt.streams.filter(only_audio=True).first()\n",
    "        video_path = video_stream.download(filename=\"video.mp4\")\n",
    "        \n",
    "        # Convert to audio file (e.g., mp3)\n",
    "        audio_path = \"audio.mp3\"\n",
    "        audio_clip = AudioFileClip(video_path)\n",
    "        audio_clip.write_audiofile(audio_path)\n",
    "        audio_clip.close()\n",
    "\n",
    "        print(\"Audio file saved as:\", audio_path)\n",
    "        break\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTPError on attempt {attempt + 1}: {e}\")\n",
    "        if attempt < max_retries - 1:\n",
    "            time.sleep(2)  # Wait 2 seconds before retrying\n",
    "        else:\n",
    "            print(\"Max retries reached. Unable to download the video.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb6802",
   "metadata": {},
   "source": [
    "LangChain can process the contents of an MP3 file by using transcription models to convert audio into text, which can then be used as context for further processing. To achieve this, you typically need to use a transcription model, like OpenAI's Whisper, Google Speech-to-Text, or other speech recognition tools to transcribe the MP3 into text. Once transcribed, the text can be used as input or context in LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a674be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yt-dlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'bestaudio/best',\n",
    "    'outtmpl': 'audio_01.mp3',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'mp3',\n",
    "        'preferredquality': '192',\n",
    "    }],\n",
    "    'ffmpeg_location': '/usr/local/bin/ffmpeg'  # Replace with your ffmpeg path if needed\n",
    "}\n",
    "\n",
    "video_url = \"https://www.youtube.com/watch?v=wDMdX-pbmxQ\"\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    ydl.download([video_url])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938b596",
   "metadata": {},
   "source": [
    "Transcribe the Audio: Use a transcription tool to convert the MP3 file to text. Below is an example with Whisper, which has high accuracy and supports multiple languages.\n",
    "\n",
    "Install Whisper (or another transcription tool):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34462d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe7a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")  # or \"large\" for better accuracy\n",
    "result = model.transcribe(\"audio.mp3\")  # Use the path to your MP3 file\n",
    "text = result['text']\n",
    "print(\"Transcription:\", text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f84ba5",
   "metadata": {},
   "source": [
    "## Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040850a",
   "metadata": {},
   "source": [
    "Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
    "\n",
    "At a high level, text splitters work as following:\n",
    "\n",
    "- Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "- Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "- Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "That means there are two different axes along which you can customize your text splitter:\n",
    "\n",
    "- How the text is split\n",
    "- How the chunk size is measured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1646a92",
   "metadata": {},
   "source": [
    "## This code snippet sets up a text splitter using the CharacterTextSplitter class from LangChain. The CharacterTextSplitter is used to break down long pieces of text into smaller chunks based on specified criteria. Let’s go through each parameter in detail\n",
    "\n",
    "\n",
    "Parameter Breakdown\n",
    "separator=\"\\n\\n\":\n",
    "\n",
    "This specifies the delimiter used to split the text. In this case, the text is split wherever there is a double newline (\\n\\n), often indicating the end of a paragraph.\n",
    "If no separator is found, the splitter will fall back to splitting at specific character counts based on chunk_size.\n",
    "chunk_size=200:\n",
    "\n",
    "This defines the maximum size of each chunk, in terms of characters. Here, each chunk will be up to 200 characters long.\n",
    "If a segment of text after splitting by the separator is longer than 200 characters, it will be further split to ensure it does not exceed the chunk size.\n",
    "chunk_overlap=20:\n",
    "\n",
    "This sets the overlap between consecutive chunks. Each chunk will have 20 characters in common with the next chunk.\n",
    "This overlap is helpful for maintaining context continuity across chunks, especially when dealing with language models that have context windows.\n",
    "length_function=len:\n",
    "\n",
    "This specifies the function used to measure the length of each chunk. Here, it uses Python’s built-in len() function, which counts characters.\n",
    "is_separator_regex=False:\n",
    "\n",
    "This parameter specifies whether the separator should be interpreted as a regular expression. Setting it to False means that the separator is treated as a simple string, not as a regex pattern.\n",
    "\n",
    "\n",
    "## The 20-character overlap means that when splitting a long text into chunks, the last 20 characters of one chunk will be repeated at the beginning of the next chunk. This overlap helps preserve context between chunks, making it easier for the model to understand the flow of information across multiple parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c14819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2b4036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://www.ibm.com/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f8e7abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(data[0].page_content)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf9015ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM - United States\n",
      "\n",
      "See, play and build with AI models for business\n",
      "----\n",
      "Meet our trusted, open IBM Granite™ models—optimized to scale your AI applications from water management to Fantasy Football\n",
      "  \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Meet Granite 3.0\n",
      "\n",
      "\n",
      "Win at Fantasy Football\n",
      "----\n",
      "Latest news\n",
      "----\n",
      "IBM \"AI in Action\" Report Identifies Key Characteristics of Businesses That Consider Themselves Leaders in AI\n",
      "----\n",
      "IBM Selected as Official Fan Engagement and Data Analytics Partner for Scuderia Ferrari HP\n",
      "----\n",
      "Cognizant Launches Global FinOps Center of Excellence, New Solutions Built with IBM Technology to Tackle Enterprise Modernization Challenges\n",
      "\n",
      "IBM BOARD APPROVES REGULAR QUARTERLY CASH DIVIDEND\n",
      "----\n",
      "IBM Brings Apptio Product Portfolio to the Microsoft Cloud to Help Organizations Make Informed Technology Planning Decisions\n",
      "----\n",
      "IBM Receives FedRAMP Authorization for its Envizi ESG Data Capture, Analysis and Reporting Solution\n",
      "\n",
      "IBM RELEASES THIRD-QUARTER RESULTS\n",
      "----\n",
      "IBM Advances Secure AI, Quantum Safe Technology with IBM Guardium Data Security Center\n",
      "\n",
      "Recommended for you\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "        \n",
      "        \n",
      "    \n",
      "\n",
      "        Save 30% on select products\n",
      "----\n",
      "Code smarter with watsonx Code Assistant\n",
      "----\n",
      "See how Water Corp. keeps the data flowing\n",
      "        \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "        \n",
      "        \n",
      "    \n",
      "\n",
      "        Power up your AI skills for free\n",
      "----\n",
      "AI insights and tools\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "For developers\n",
      "----\n",
      "For developers\n",
      "\n",
      "Grow your skills and create something new with our AI tools and foundation models. Then connect, collaborate and innovate with your peers.\n",
      "\n",
      "\n",
      "Start building with IBM Granite 3.0 models\n",
      "----\n",
      "Explore AI courses, APIs, data sets and more\n",
      "\n",
      "Accelerate software development with watsonx Code Assistant\n",
      "\n",
      "Check out the watsonx.ai Developer Toolkit\n",
      "\n",
      "\n",
      "For business leaders\n",
      "----\n",
      "Transform business and drive growth with AI tools, technology and insights that help you stay competitive—and responsibly map your organization's future. \n",
      "\n",
      "\n",
      "Read the CEO's guide to generative AI\n",
      "----\n",
      "Read the AI in Action report\n",
      "\n",
      "Deploy an AI agent for HR with watsonx Orchestrate\n",
      "\n",
      "Protect your data with IBM Guardium Data Security Center\n",
      "\n",
      "\n",
      "Save 30% this November\n",
      "----\n",
      "Whether you’re just starting or well into your enterprise AI journey, accelerate your business success with special offers on select IBM products\n",
      "\n",
      "\n",
      "See current promotions\n",
      "\n",
      "\n",
      "Technology & Consulting\n",
      "----\n",
      "From next-generation AI to cutting edge hybrid cloud solutions to the deep expertise of IBM Consulting, IBM has what it takes to help you reinvent how your business works in the age of AI.\n",
      "----\n",
      "AI solutions\n",
      "\n",
      "Go from AI pilots to production with AI technologies built for business\n",
      "\n",
      "\n",
      "                    \n",
      "                    AI models\n",
      "----\n",
      "Get started with cost-efficient AI models, tailored for business and optimized for scale\n",
      "\n",
      "\n",
      "                    \n",
      "                    Consulting\n",
      "----\n",
      "Engage with IBM Consulting to design, build and operate high-performing businesses\n",
      "\n",
      "\n",
      "                    \n",
      "                    Analytics\n",
      "\n",
      "Support data-driven decisions for your business\n",
      "----\n",
      "IT automation\n",
      "\n",
      "Discover how automation solutions increase productivity while managing costs\n",
      "\n",
      "\n",
      "                    \n",
      "                    Compute & servers\n",
      "----\n",
      "Handle mission-critical workloads while maintaining security, reliability and control of your entire IT infrastructure\n",
      "\n",
      "\n",
      "                    \n",
      "                    Databases\n",
      "----\n",
      "Run your applications, analytics and generative AI with databases on any cloud\n",
      "\n",
      "\n",
      "                    \n",
      "                    Security & identity\n",
      "----\n",
      "Secure hybrid cloud and AI with data and identity-centric cybersecurity solutions\n",
      "\n",
      "\n",
      "            \n",
      "            \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "    Inside IBM\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "  \n",
      "    Our company\n",
      "----\n",
      "Explore IBM history and culture of putting technology to work in the real world\n",
      "\n",
      "About IBM\n",
      "            \n",
      "        \n",
      "Our history\n",
      "            \n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "  \n",
      "    Our impact\n",
      "----\n",
      "Learn about IBM's commitment to environmental, equitable and ethical pillars\n",
      "\n",
      "Corporate social responsibility\n",
      "            \n",
      "        \n",
      "Diversity and inclusion\n",
      "----\n",
      "Our innovations\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Visit the IBM lab, and see what's in store for the future of computing\n",
      "\n",
      "IBM Research\n",
      "            \n",
      "        \n",
      "Quantum computing\n",
      "----\n",
      "Take the next step\n",
      "----\n",
      "Solving the world’s problems through technology wouldn’t be possible without people with the right skills. See what it takes to become an IBMer, or build your skills with our educational courses.\n",
      "----\n",
      "Become an IBMer\n",
      "        \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "        \n",
      "            Explore jobs\n",
      "----\n",
      "Explore learning opportunities\n",
      "        \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "        \n",
      "            Start learning\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a56b6dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = text_splitter.create_documents([data[0].page_content])\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d818f678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='IBM - United States\n",
      "\n",
      "See, play and build with AI models for business'\n",
      "----\n",
      "page_content='Meet our trusted, open IBM Granite™ models—optimized to scale your AI applications from water management to Fantasy Football\n",
      "  \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Meet Granite 3.0\n",
      "\n",
      "\n",
      "Win at Fantasy Football'\n",
      "----\n",
      "page_content='Latest news'\n",
      "----\n",
      "page_content='IBM \"AI in Action\" Report Identifies Key Characteristics of Businesses That Consider Themselves Leaders in AI'\n",
      "----\n",
      "page_content='IBM Selected as Official Fan Engagement and Data Analytics Partner for Scuderia Ferrari HP'\n",
      "----\n",
      "page_content='Cognizant Launches Global FinOps Center of Excellence, New Solutions Built with IBM Technology to Tackle Enterprise Modernization Challenges\n",
      "\n",
      "IBM BOARD APPROVES REGULAR QUARTERLY CASH DIVIDEND'\n",
      "----\n",
      "page_content='IBM Brings Apptio Product Portfolio to the Microsoft Cloud to Help Organizations Make Informed Technology Planning Decisions'\n",
      "----\n",
      "page_content='IBM Receives FedRAMP Authorization for its Envizi ESG Data Capture, Analysis and Reporting Solution\n",
      "\n",
      "IBM RELEASES THIRD-QUARTER RESULTS'\n",
      "----\n",
      "page_content='IBM Advances Secure AI, Quantum Safe Technology with IBM Guardium Data Security Center\n",
      "\n",
      "Recommended for you\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "        \n",
      "        \n",
      "    \n",
      "\n",
      "        Save 30% on select products'\n",
      "----\n",
      "page_content='Code smarter with watsonx Code Assistant'\n",
      "----\n",
      "page_content='See how Water Corp. keeps the data flowing\n",
      "        \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "        \n",
      "        \n",
      "    \n",
      "\n",
      "        Power up your AI skills for free'\n",
      "----\n",
      "page_content='AI insights and tools\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "For developers'\n",
      "----\n",
      "page_content='For developers\n",
      "\n",
      "Grow your skills and create something new with our AI tools and foundation models. Then connect, collaborate and innovate with your peers.\n",
      "\n",
      "\n",
      "Start building with IBM Granite 3.0 models'\n",
      "----\n",
      "page_content='Explore AI courses, APIs, data sets and more\n",
      "\n",
      "Accelerate software development with watsonx Code Assistant\n",
      "\n",
      "Check out the watsonx.ai Developer Toolkit\n",
      "\n",
      "\n",
      "For business leaders'\n",
      "----\n",
      "page_content='Transform business and drive growth with AI tools, technology and insights that help you stay competitive—and responsibly map your organization's future. \n",
      "\n",
      "\n",
      "Read the CEO's guide to generative AI'\n",
      "----\n",
      "page_content='Read the AI in Action report\n",
      "\n",
      "Deploy an AI agent for HR with watsonx Orchestrate\n",
      "\n",
      "Protect your data with IBM Guardium Data Security Center\n",
      "\n",
      "\n",
      "Save 30% this November'\n",
      "----\n",
      "page_content='Whether you’re just starting or well into your enterprise AI journey, accelerate your business success with special offers on select IBM products\n",
      "\n",
      "\n",
      "See current promotions\n",
      "\n",
      "\n",
      "Technology & Consulting'\n",
      "----\n",
      "page_content='From next-generation AI to cutting edge hybrid cloud solutions to the deep expertise of IBM Consulting, IBM has what it takes to help you reinvent how your business works in the age of AI.'\n",
      "----\n",
      "page_content='AI solutions\n",
      "\n",
      "Go from AI pilots to production with AI technologies built for business\n",
      "\n",
      "\n",
      "                    \n",
      "                    AI models'\n",
      "----\n",
      "page_content='Get started with cost-efficient AI models, tailored for business and optimized for scale\n",
      "\n",
      "\n",
      "                    \n",
      "                    Consulting'\n",
      "----\n",
      "page_content='Engage with IBM Consulting to design, build and operate high-performing businesses\n",
      "\n",
      "\n",
      "                    \n",
      "                    Analytics\n",
      "\n",
      "Support data-driven decisions for your business'\n",
      "----\n",
      "page_content='IT automation\n",
      "\n",
      "Discover how automation solutions increase productivity while managing costs\n",
      "\n",
      "\n",
      "                    \n",
      "                    Compute & servers'\n",
      "----\n",
      "page_content='Handle mission-critical workloads while maintaining security, reliability and control of your entire IT infrastructure\n",
      "\n",
      "\n",
      "                    \n",
      "                    Databases'\n",
      "----\n",
      "page_content='Run your applications, analytics and generative AI with databases on any cloud\n",
      "\n",
      "\n",
      "                    \n",
      "                    Security & identity'\n",
      "----\n",
      "page_content='Secure hybrid cloud and AI with data and identity-centric cybersecurity solutions\n",
      "\n",
      "\n",
      "            \n",
      "            \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "    Inside IBM\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "  \n",
      "    Our company'\n",
      "----\n",
      "page_content='Explore IBM history and culture of putting technology to work in the real world\n",
      "\n",
      "About IBM\n",
      "            \n",
      "        \n",
      "Our history\n",
      "            \n",
      "        \n",
      "\n",
      "        \n",
      "\n",
      "  \n",
      "    Our impact'\n",
      "----\n",
      "page_content='Learn about IBM's commitment to environmental, equitable and ethical pillars\n",
      "\n",
      "Corporate social responsibility\n",
      "            \n",
      "        \n",
      "Diversity and inclusion'\n",
      "----\n",
      "page_content='Our innovations\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "Visit the IBM lab, and see what's in store for the future of computing\n",
      "\n",
      "IBM Research\n",
      "            \n",
      "        \n",
      "Quantum computing'\n",
      "----\n",
      "page_content='Take the next step'\n",
      "----\n",
      "page_content='Solving the world’s problems through technology wouldn’t be possible without people with the right skills. See what it takes to become an IBMer, or build your skills with our educational courses.'\n",
      "----\n",
      "page_content='Become an IBMer\n",
      "        \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "        \n",
      "            Explore jobs'\n",
      "----\n",
      "page_content='Explore learning opportunities\n",
      "        \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "        \n",
      "            Start learning'\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da9b7eb",
   "metadata": {},
   "source": [
    "## RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fc55f",
   "metadata": {},
   "source": [
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.\n",
    "- How the text is split: by list of characters.\n",
    "- How the chunk size is measured: by number of characters.\n",
    "- The RecursiveCharacterTextSplitter class does use chunk_size and overlap parameters to split the text into chunks of the specified size and overlap. This is because its split_text method recursively splits the text based on different separators until the length of the splits is less than the chunk_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "38234668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "rectext_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e1f1ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = rectext_splitter.create_documents([data[0].page_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    print(text)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026c9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl \\\n",
    "-X DELETE \\\n",
    "-H \"Authorization: Bearer 2omELGyL9RxDOQBVn6xoSmPNIlz_5h3ehmPKehPr8LvcpXKwR\" \\\n",
    "-H \"Ngrok-Version: 2\" \\\n",
    "https://api.ngrok.com/endpoints/op_2omFI7Phqf5PSSKyUPM8Tvx0nVH\n",
    "\n",
    "\n",
    "curl \\\n",
    "-X GET \\\n",
    "-H \"Authorization: Bearer 2omELGyL9RxDOQBVn6xoSmPNIlz_5h3ehmPKehPr8LvcpXKwR\" \\\n",
    "-H \"Ngrok-Version: 2\" \\\n",
    "https://api.ngrok.com/endpoints"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
