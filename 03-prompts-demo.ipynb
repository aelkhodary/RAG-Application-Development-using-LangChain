{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e515b6",
   "metadata": {},
   "source": [
    "## Getting Started with prompt Template and Chat Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e27cd0",
   "metadata": {},
   "source": [
    "#### Use PromptTemplate to create a template for a string prompt.\n",
    "By default, PromptTemplate uses Python’s str.format syntax for templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "71fc036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "from dotenv import load_dotenv\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b963a811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "467b6c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.5)\n",
    "#model = ChatGoogleGenerativeAI(model=\"gemini-pro\",  google_api_key=os.getenv(\"GOOGLE_API_KEY\"), temperature=0.8)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-002\",  google_api_key=os.getenv(\"GOOGLE_API_KEY\"), temperature=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf814975",
   "metadata": {},
   "source": [
    "### A simple string based Prompt formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fc64f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response one: Why did the British man bring a ladder to the beach?\n",
      "\n",
      "Because he heard the forecast was partly cloudy.  He was hoping to reach the sunny part.\n",
      "\n",
      "Response two: Why was the cat sitting on the computer?  \n",
      "\n",
      "To keep an eye on the mouse!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "#print(prompt_template.__doc__)\n",
    "prompt = prompt_template.format(adjective=\"sad\", content=\"British Weather\")\n",
    "\n",
    "response_one = model.invoke(prompt)\n",
    "print(f'Response one: {response_one.content}')\n",
    "\n",
    "# Create the Chain\n",
    "chain = prompt_template | model\n",
    "response_two = chain.invoke({\"adjective\": \"funny\", \"content\": \"cats\"})\n",
    "print(f\"Response two: {response_two.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997af107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4ce0254",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate: The prompt to chat models is a list of chat messages.\n",
    "Each chat message is associated with content, and an additional parameter called role. <br>\n",
    "For example, in the OpenAI Chat Completions API, a chat message can be associated with an AI assistant, a human or a system role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "611c91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57a2d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Final Prompt was: You are a helpful AI bot. Your name is Bob. Hello, how are you doing? I'm doing well, thanks! What is your name and what are you good at?\n",
      "The response : ******************\n",
      "My name is Bob.  I'm good at a variety of things, including:\n",
      "\n",
      "* **Understanding and generating human-like text:** I can answer your questions, write different kinds of creative content, and translate languages.\n",
      "* **Following instructions and completing tasks as instructed:**  Tell me what you need, and I'll do my best to help.\n",
      "* **Accessing and processing information:** I can draw on a massive dataset to provide you with information on a wide range of topics.\n",
      "* **Learning and adapting:**  I'm constantly learning and improving my abilities.\n",
      "\n",
      "What can I help you with today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The conversation in the template is structured like this:\n",
    "\n",
    "System Message: \"You are a helpful AI bot. Your name is {name}.\"\n",
    "\n",
    "This is used to give context to the AI assistant.\n",
    "Role: \"system\" – provides configuration and context for the AI model.\n",
    "Content: Introduces the bot as a helpful AI with a placeholder for its name.\n",
    "Human Message: \"Hello, how are you doing?\"\n",
    "\n",
    "The human starts the conversation with a greeting.\n",
    "Role: \"human\" – represents the human user speaking to the AI.\n",
    "AI Message: \"I'm doing well, thanks!\"\n",
    "\n",
    "The AI replies to the human.\n",
    "Role: \"ai\" – represents the AI’s response to the initial human message.\n",
    "Human Message: \"{user_input}\"\n",
    "\n",
    "The human follows up with another input that will be provided dynamically.\n",
    "Role: \"human\" – this represents the next message in the conversation, and it contains \n",
    "a placeholder ({user_input}) that will be filled with a real question or input later.\n",
    "'''\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\"),\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "The .format_messages() method is used to replace placeholders with real values.\n",
    " This helps create the actual content that will be sent to the model.\n",
    "Parameters:\n",
    "name=\"Bob\": This fills in the {name} placeholder, so the AI will refer to itself as \"Bob\".\n",
    "user_input=\"What is your name and what are you good at?\": This fills in the {user_input}\n",
    " placeholder with the actual question the human asks.\n",
    "'''\n",
    "prompt = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name and what are you good at?\")\n",
    "\n",
    "formatted_prompt_text = \" \".join([message.content for message in prompt])\n",
    "print(f'The Final Prompt was: {formatted_prompt_text}')\n",
    "response = model.invoke(prompt)\n",
    "print('The response : ******************')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ef76f",
   "metadata": {},
   "source": [
    "### Various ways of formatting System/Human/AI prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af82cb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm discovering new foods and having fun trying different flavors!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that re-writes the user's text to \"\n",
    "                \"sound more upbeat.\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    ")\n",
    "prompt = chat_template.format_messages(text=\"I don't like eating tasty things\")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282c0e5e",
   "metadata": {},
   "source": [
    "### Providing a Context along with the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5b7973c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs?\n",
    "\n",
    "Answer: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73cb2a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hugging Face's `transformers` library, OpenAI's `openai` library, and Cohere's `cohere` library offer LLMs.\\n\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-61ed59f9-b69e-4578-86ba-f87926c8db0b-0' usage_metadata={'input_tokens': 126, 'output_tokens': 35, 'total_tokens': 161}\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f00e1d",
   "metadata": {},
   "source": [
    "### Langchain’s ```FewShotPromptTemplate``` caters to source knowledge input. The idea is to “train” the model on a few examples — we call this few-shot learning — and these examples are given to the model within the prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b8f66",
   "metadata": {},
   "source": [
    "The goal of few-shot prompt templates are to dynamically select examples based on an input, and then format the examples in a final prompt to provide for the model.<br>\n",
    "\n",
    "<b>Fixed Examples</b><br>\n",
    "The most basic (and common) few-shot prompting technique is to use a fixed prompt example. This way you can select a chain, evaluate it, and avoid worrying about additional moving parts in production. <br>\n",
    "\n",
    "The basic components of the template are: - examples: A list of dictionary examples to include in the final prompt. - example_prompt: converts each example into 1 or more messages through its format_messages method. A common example would be to convert each example into one human message and one AI message response, or a human message followed by a function call message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15e22ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, \n",
    "    {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09912e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt example from above template\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d23b00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8c7b333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: What movie should I watch this evening?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What movie should I watch this evening?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29ca44a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Anything but the one you'll inevitably fall asleep during.  Unless that's your goal, in which case, I recommend a nature documentary.\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = few_shot_prompt_template | model\n",
    "\n",
    "chain.invoke({\"query\": query}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46e631",
   "metadata": {},
   "source": [
    "```Summary```\n",
    "\n",
    "1-FewShotPromptTemplate is used to generate prompts that include a few examples to show the model the expected format and tone.\n",
    "\n",
    "2-It is composed of prefix (instructions), few-shot examples, and suffix (the current user query).\n",
    "\n",
    "3-This prompt helps the model generate responses that align with the tone and style of the few-shot examples.\n",
    "\n",
    "4-It is particularly useful in few-shot learning settings where you want the model to generate consistent and contextually relevant responses based on limited examples.\n",
    "\n",
    "This kind of template can be extremely effective in helping the model understand the desired output style and personality without explicitly training the model on a new dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
