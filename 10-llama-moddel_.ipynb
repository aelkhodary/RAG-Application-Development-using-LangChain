{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79b56b93-4a7d-4fd2-b103-3eb89cb1e408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/user/miniconda/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: langchain in /home/user/miniconda/lib/python3.9/site-packages (0.3.8)\n",
      "Requirement already satisfied: langchain-community in /home/user/miniconda/lib/python3.9/site-packages (0.3.8)\n",
      "Requirement already satisfied: langchain-core in /home/user/miniconda/lib/python3.9/site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-google-genai in /home/user/miniconda/lib/python3.9/site-packages (2.0.5)\n",
      "Requirement already satisfied: langchain-openai in /home/user/miniconda/lib/python3.9/site-packages (0.2.9)\n",
      "Requirement already satisfied: langchain-text-splitters in /home/user/miniconda/lib/python3.9/site-packages (0.3.2)\n",
      "Requirement already satisfied: huggingface_hub in /home/user/miniconda/lib/python3.9/site-packages (0.26.2)\n",
      "Requirement already satisfied: transformers in /home/user/miniconda/lib/python3.9/site-packages (4.46.3)\n",
      "Requirement already satisfied: jinja2 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: networkx in /home/user/miniconda/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: fsspec in /home/user/miniconda/lib/python3.9/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: filelock in /home/user/miniconda/lib/python3.9/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/user/miniconda/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/user/miniconda/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/user/miniconda/lib/python3.9/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/user/miniconda/lib/python3.9/site-packages (from langchain) (2.10.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/user/miniconda/lib/python3.9/site-packages (from langchain) (3.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/user/miniconda/lib/python3.9/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/user/miniconda/lib/python3.9/site-packages (from langchain) (0.1.145)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/user/miniconda/lib/python3.9/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/user/miniconda/lib/python3.9/site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/user/miniconda/lib/python3.9/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/user/miniconda/lib/python3.9/site-packages (from langchain-core) (24.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/miniconda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/user/miniconda/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/user/miniconda/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.12)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/user/miniconda/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/user/miniconda/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: idna in /home/user/miniconda/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (2.10)\n",
      "Requirement already satisfied: certifi in /home/user/miniconda/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (2021.5.30)\n",
      "Requirement already satisfied: sniffio in /home/user/miniconda/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: anyio in /home/user/miniconda/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/user/miniconda/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/user/miniconda/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/user/miniconda/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/miniconda/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/miniconda/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.6)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/user/miniconda/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/user/miniconda/lib/python3.9/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/user/miniconda/lib/python3.9/site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /home/user/miniconda/lib/python3.9/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/user/miniconda/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/user/miniconda/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/user/miniconda/lib/python3.9/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/user/miniconda/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in /home/user/miniconda/lib/python3.9/site-packages (from langchain-google-genai) (0.8.3)\n",
      "Requirement already satisfied: protobuf in /home/user/miniconda/lib/python3.9/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.28.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /home/user/miniconda/lib/python3.9/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in /home/user/miniconda/lib/python3.9/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.23.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /home/user/miniconda/lib/python3.9/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.36.0)\n",
      "Requirement already satisfied: tqdm in /home/user/miniconda/lib/python3.9/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.61.2)\n",
      "Requirement already satisfied: google-api-python-client in /home/user/miniconda/lib/python3.9/site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.154.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/user/miniconda/lib/python3.9/site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.25.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/user/miniconda/lib/python3.9/site-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.66.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/user/miniconda/lib/python3.9/site-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.68.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/user/miniconda/lib/python3.9/site-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.68.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/user/miniconda/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/user/miniconda/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/user/miniconda/lib/python3.9/site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/user/miniconda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /home/user/miniconda/lib/python3.9/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /home/user/miniconda/lib/python3.9/site-packages (from langchain-openai) (1.55.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/user/miniconda/lib/python3.9/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/user/miniconda/lib/python3.9/site-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/user/miniconda/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/user/miniconda/lib/python3.9/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user/miniconda/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/user/miniconda/lib/python3.9/site-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /home/user/miniconda/lib/python3.9/site-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/user/miniconda/lib/python3.9/site-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/user/miniconda/lib/python3.9/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/miniconda/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch langchain langchain-community langchain-core langchain-google-genai langchain-openai langchain-text-splitters huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40f7df-cf12-46fe-919b-6350fe744a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blobfile tiktoken SentencePiece accelerate>=0.26.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3a877-40bf-46db-855e-c54ddc950222",
   "metadata": {},
   "source": [
    "# This program demonstrates a simple usage of creating a LCEL based chain \n",
    "# The chain comprises a prompt, the llm object and a Stringoutput parser\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "# Create the Hugging Face Hub LLM Object\n",
    "# Hugging Face Hub LLM\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"meta-llama/Llama-2-7B\",\n",
    "    huggingfacehub_api_token=\"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\",\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 50},\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a world-class technical documentation writer.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92b026-a975-4040-bbeb-59ce2a092a49",
   "metadata": {},
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import os\n",
    "\n",
    "# # Define the model name\n",
    "# model_name = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "# # Retrieve the Hugging Face token from an environment variable\n",
    "# huggingface_token = \"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\"\n",
    "\n",
    "# # Load the tokenizer and model with the token\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=huggingface_token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=\"float16\",\n",
    "#     use_auth_token=huggingface_token\n",
    "# )\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "# Explicitly use the SentencePiece tokenizer if required\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"float16\",\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306283a9-0316-4887-9f82-0a47516f5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "# model_path = 'openlm-research/open_llama_7b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "\n",
    "prompt = 'Q: What is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=32\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93509a90-4327-4a3b-92f1-d5311f85c284",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the local Hugging Face model\n",
    "class LocalLLM(BaseLanguageModel):\n",
    "    def __init__(self, model_path: str, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the LocalLLM with the model path and optional model parameters.\n",
    "        \"\"\"\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            **model_kwargs\n",
    "        )\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenize the input prompt, generate a response, and decode the output.\n",
    "        \"\"\"\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        # Generate response\n",
    "        outputs = self.model.generate(inputs.input_ids, max_new_tokens=50, temperature=0.7)\n",
    "        # Decode and return\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3408a-770a-407d-a29b-87a64860bbd9",
   "metadata": {},
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class LocalLLM(BaseLanguageModel):\n",
    "    model_path: str  # Define model path as a Pydantic field\n",
    "    tokenizer: LlamaTokenizer = Field(init=False)  # Declare tokenizer\n",
    "    model: LlamaForCausalLM = Field(init=False)  # Declare model\n",
    "\n",
    "    def __init__(self, model_path: str, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the LocalLLM with the model path and optional model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__(model_path=model_path)  # Initialize Pydantic BaseModel\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            **model_kwargs\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: dict) -> str:\n",
    "        \"\"\"\n",
    "        Main method required by BaseLanguageModel to process inputs and produce outputs.\n",
    "        Expects a dictionary with a single key 'input'.\n",
    "        \"\"\"\n",
    "        prompt = input.get(\"input\", \"\")\n",
    "        if not prompt:\n",
    "            raise ValueError(\"Input dictionary must contain a key 'input' with a valid prompt string.\")\n",
    "        return self.generate_prompt(prompt)\n",
    "\n",
    "    def generate_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response for a single prompt.\n",
    "        \"\"\"\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        # Generate response\n",
    "        outputs = self.model.generate(inputs.input_ids, max_new_tokens=50, temperature=0.7)\n",
    "        # Decode and return\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Alias for generate_prompt to comply with LangChain's API.\n",
    "        \"\"\"\n",
    "        return self.generate_prompt(text)\n",
    "\n",
    "    def predict_messages(self, messages: list) -> str:\n",
    "        \"\"\"\n",
    "        Process a list of messages (e.g., for chat-style prompts).\n",
    "        \"\"\"\n",
    "        # Combine all messages into a single input prompt\n",
    "        combined_prompt = \"\\n\".join(message[\"content\"] for message in messages)\n",
    "        return self.generate_prompt(combined_prompt)\n",
    "\n",
    "    async def agenerate_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronous method for generating a response to a prompt.\n",
    "        \"\"\"\n",
    "        return self.generate_prompt(prompt)\n",
    "\n",
    "    async def apredict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronous alias for predict.\n",
    "        \"\"\"\n",
    "        return await self.agenerate_prompt(text)\n",
    "\n",
    "    async def apredict_messages(self, messages: list) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronous method for processing chat-style prompts.\n",
    "        \"\"\"\n",
    "        combined_prompt = \"\\n\".join(message[\"content\"] for message in messages)\n",
    "        return await self.agenerate_prompt(combined_prompt)\n",
    "\n",
    "\n",
    "\n",
    "# Rebuild the Pydantic model to resolve the \"not fully defined\" error\n",
    "LocalLLM.model_rebuild()\n",
    "# Specify the model path\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "\n",
    "# Initialize the LocalLLM\n",
    "llm = LocalLLM(model_path=model_path)\n",
    "\n",
    "# Test with a single prompt\n",
    "prompt = \"Q: What is the largest animal?\\nA:\"\n",
    "response = llm.invoke({\"input\": prompt})\n",
    "print(\"Response:\", response)\n",
    "\n",
    "# Test with chat-style messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the largest animal on Earth?\"}\n",
    "]\n",
    "response = llm.predict_messages(messages)\n",
    "print(\"Chat Response:\", response)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9592c7-0c3c-4096-8820-87dd8da51ae6",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "class LanguageModel:\n",
    "    def __init__(self, model_path: str, device: str = \"auto\", torch_dtype=torch.float16):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer and model.\n",
    "\n",
    "        :param model_path: Path to the pre-trained model.\n",
    "        :param device: Device map for loading the model. Default is 'auto'.\n",
    "        :param torch_dtype: Data type for model parameters. Default is torch.float16.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the tokenizer and model.\"\"\"\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path)\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            torch_dtype=self.torch_dtype,\n",
    "            device_map=self.device,\n",
    "        )\n",
    "        print(f\"Model and tokenizer loaded from {self.model_path}\")\n",
    "        return self  # Ensure the object is returned for chaining\n",
    "\n",
    "    def generate_response(self, prompt: str, max_new_tokens: int = 32) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response for a given prompt.\n",
    "\n",
    "        :param prompt: Input prompt string.\n",
    "        :param max_new_tokens: Maximum number of new tokens to generate.\n",
    "        :return: Generated response as a string.\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            raise ValueError(\"Model and tokenizer must be loaded first using load_model().\")\n",
    "        \n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        output_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = 'openlm-research/open_llama_3b'  # Change to your desired model\n",
    "    lm = LanguageModel(model_path)\n",
    "    lm.load_model()\n",
    "    prompt = \"Q: What is the largest animal?\\nA:\"\n",
    "    response = lm.generate_response(prompt)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c91993c-6b0b-42c2-8dda-ac03a0be5241",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from langchain_core.runnables.base import Runnable\n",
    "\n",
    "class LanguageModel(Runnable):\n",
    "    def __init__(self, model_path: str, device: str = \"auto\", torch_dtype=torch.float16):\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the tokenizer and model.\"\"\"\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path)\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            torch_dtype=self.torch_dtype,\n",
    "            device_map=self.device,\n",
    "        )\n",
    "        print(f\"Model and tokenizer loaded from {self.model_path}\")\n",
    "        return self  # Ensure the object is returned for chaining\n",
    "\n",
    "    def invoke(self, input: dict, config=None) -> str:\n",
    "        \"\"\"\n",
    "        Makes the model runnable by implementing the `invoke` method.\n",
    "        Accepts a dictionary input from LangChain's pipeline.\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            raise ValueError(\"Model and tokenizer must be loaded first using load_model().\")\n",
    "\n",
    "        # Extract the input text from the dictionary\n",
    "        # if \"input\" not in input:\n",
    "        #     raise ValueError(\"Expected a dictionary with an 'input' key.\")\n",
    "        \n",
    "        prompt = input[\"input\"]  # Extract the actual string input\n",
    "    \n",
    "    \n",
    "        # Tokenize input\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "        # Generate output\n",
    "        output_ids = self.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=32\n",
    "        )\n",
    "        \n",
    "        # Decode and return response\n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1057d686-b060-4080-8978-a0b9f4f198c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from openlm-research/open_llama_3b\n"
     ]
    }
   ],
   "source": [
    "model_path = 'openlm-research/open_llama_3b'  # Change to your desired model\n",
    "llm = LanguageModel(model_path).load_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a51d4127-a00d-47cd-ba5b-fc420c6c7496",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded from openlm-research/open_llama_3b\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'ChatPromptValue' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm \u001b[38;5;241m|\u001b[39m output_parser\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Run the chain\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhow can langsmith help with testing?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[0;32m~/miniconda/lib/python3.9/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[39], line 36\u001b[0m, in \u001b[0;36mLanguageModel.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel and tokenizer must be loaded first using load_model().\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Extract the input text from the dictionary\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# if \"input\" not in input:\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     raise ValueError(\"Expected a dictionary with an 'input' key.\")\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Extract the actual string input\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Tokenize input\u001b[39;00m\n\u001b[1;32m     40\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ChatPromptValue' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the model path\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "\n",
    "# Load the language model\n",
    "llm = LanguageModel(model_path).load_model()\n",
    "\n",
    "# Define the prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Define the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create the chain (prompt | llm | output_parser)\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Run the chain\n",
    "output = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8720d7-3e15-41a7-85f2-eff64b3383e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
