{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b56b93-4a7d-4fd2-b103-3eb89cb1e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch langchain langchain-community langchain-core langchain-google-genai langchain-openai langchain-text-splitters huggingface_hub transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca40f7df-cf12-46fe-919b-6350fe744a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 0.26.0 not found\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install blobfile tiktoken sentencePiece accelerate>=0.26.0 SentencePiece safetensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3a877-40bf-46db-855e-c54ddc950222",
   "metadata": {},
   "source": [
    "# This program demonstrates a simple usage of creating a LCEL based chain \n",
    "# The chain comprises a prompt, the llm object and a Stringoutput parser\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import HuggingFaceHub\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "# Create the Hugging Face Hub LLM Object\n",
    "# Hugging Face Hub LLM\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"meta-llama/Llama-2-7B\",\n",
    "    huggingfacehub_api_token=\"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\",\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 50},\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a world-class technical documentation writer.\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "output = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92b026-a975-4040-bbeb-59ce2a092a49",
   "metadata": {},
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# import os\n",
    "\n",
    "# # Define the model name\n",
    "# model_name = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "# # Retrieve the Hugging Face token from an environment variable\n",
    "# huggingface_token = \"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\"\n",
    "\n",
    "# # Load the tokenizer and model with the token\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=huggingface_token)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=\"float16\",\n",
    "#     use_auth_token=huggingface_token\n",
    "# )\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"openlm-research/open_llama_3b\"\n",
    "\n",
    "# Explicitly use the SentencePiece tokenizer if required\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"float16\",\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306283a9-0316-4887-9f82-0a47516f5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "#model_path = '/Users/aelkhodary/Documents/GitHub/Books/DATA/LLM/openai-generative-ai/model'   # Change to your desired model openlm-research/open_llama_7b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_path, torch_dtype=torch.float16, device_map='auto',\n",
    ")\n",
    "\n",
    "prompt = 'Q: What is the largest animal?\\nA:'\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "generation_output = model.generate(\n",
    "    input_ids=input_ids, max_new_tokens=32\n",
    ")\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93509a90-4327-4a3b-92f1-d5311f85c284",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "# Load the local Hugging Face model\n",
    "class LocalLLM(BaseLanguageModel):\n",
    "    def __init__(self, model_path: str, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the LocalLLM with the model path and optional model parameters.\n",
    "        \"\"\"\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            **model_kwargs\n",
    "        )\n",
    "\n",
    "    def invoke(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenize the input prompt, generate a response, and decode the output.\n",
    "        \"\"\"\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        # Generate response\n",
    "        outputs = self.model.generate(inputs.input_ids, max_new_tokens=50, temperature=0.7)\n",
    "        # Decode and return\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3408a-770a-407d-a29b-87a64860bbd9",
   "metadata": {},
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.base_language import BaseLanguageModel\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import torch\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class LocalLLM(BaseLanguageModel):\n",
    "    model_path: str  # Define model path as a Pydantic field\n",
    "    tokenizer: LlamaTokenizer = Field(init=False)  # Declare tokenizer\n",
    "    model: LlamaForCausalLM = Field(init=False)  # Declare model\n",
    "\n",
    "    def __init__(self, model_path: str, **model_kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the LocalLLM with the model path and optional model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__(model_path=model_path)  # Initialize Pydantic BaseModel\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            **model_kwargs\n",
    "        )\n",
    "\n",
    "    def invoke(self, input: dict) -> str:\n",
    "        \"\"\"\n",
    "        Main method required by BaseLanguageModel to process inputs and produce outputs.\n",
    "        Expects a dictionary with a single key 'input'.\n",
    "        \"\"\"\n",
    "        prompt = input.get(\"input\", \"\")\n",
    "        if not prompt:\n",
    "            raise ValueError(\"Input dictionary must contain a key 'input' with a valid prompt string.\")\n",
    "        return self.generate_prompt(prompt)\n",
    "\n",
    "    def generate_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response for a single prompt.\n",
    "        \"\"\"\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        # Generate response\n",
    "        outputs = self.model.generate(inputs.input_ids, max_new_tokens=50, temperature=0.7)\n",
    "        # Decode and return\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Alias for generate_prompt to comply with LangChain's API.\n",
    "        \"\"\"\n",
    "        return self.generate_prompt(text)\n",
    "\n",
    "    def predict_messages(self, messages: list) -> str:\n",
    "        \"\"\"\n",
    "        Process a list of messages (e.g., for chat-style prompts).\n",
    "        \"\"\"\n",
    "        # Combine all messages into a single input prompt\n",
    "        combined_prompt = \"\\n\".join(message[\"content\"] for message in messages)\n",
    "        return self.generate_prompt(combined_prompt)\n",
    "\n",
    "    async def agenerate_prompt(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronous method for generating a response to a prompt.\n",
    "        \"\"\"\n",
    "        return self.generate_prompt(prompt)\n",
    "\n",
    "    async def apredict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronous alias for predict.\n",
    "        \"\"\"\n",
    "        return await self.agenerate_prompt(text)\n",
    "\n",
    "    async def apredict_messages(self, messages: list) -> str:\n",
    "        \"\"\"\n",
    "        Asynchronous method for processing chat-style prompts.\n",
    "        \"\"\"\n",
    "        combined_prompt = \"\\n\".join(message[\"content\"] for message in messages)\n",
    "        return await self.agenerate_prompt(combined_prompt)\n",
    "\n",
    "\n",
    "\n",
    "# Rebuild the Pydantic model to resolve the \"not fully defined\" error\n",
    "LocalLLM.model_rebuild()\n",
    "# Specify the model path\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "\n",
    "# Initialize the LocalLLM\n",
    "llm = LocalLLM(model_path=model_path)\n",
    "\n",
    "# Test with a single prompt\n",
    "prompt = \"Q: What is the largest animal?\\nA:\"\n",
    "response = llm.invoke({\"input\": prompt})\n",
    "print(\"Response:\", response)\n",
    "\n",
    "# Test with chat-style messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the largest animal on Earth?\"}\n",
    "]\n",
    "response = llm.predict_messages(messages)\n",
    "print(\"Chat Response:\", response)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9592c7-0c3c-4096-8820-87dd8da51ae6",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "class LanguageModel:\n",
    "    def __init__(self, model_path: str, device: str = \"auto\", torch_dtype=torch.float16):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer and model.\n",
    "\n",
    "        :param model_path: Path to the pre-trained model.\n",
    "        :param device: Device map for loading the model. Default is 'auto'.\n",
    "        :param torch_dtype: Data type for model parameters. Default is torch.float16.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the tokenizer and model.\"\"\"\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path)\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            torch_dtype=self.torch_dtype,\n",
    "            device_map=self.device,\n",
    "        )\n",
    "        print(f\"Model and tokenizer loaded from {self.model_path}\")\n",
    "        return self  # Ensure the object is returned for chaining\n",
    "\n",
    "    def generate_response(self, prompt: str, max_new_tokens: int = 32) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response for a given prompt.\n",
    "\n",
    "        :param prompt: Input prompt string.\n",
    "        :param max_new_tokens: Maximum number of new tokens to generate.\n",
    "        :return: Generated response as a string.\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            raise ValueError(\"Model and tokenizer must be loaded first using load_model().\")\n",
    "        \n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        output_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return response\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = 'openlm-research/open_llama_3b'  # Change to your desired model\n",
    "    lm = LanguageModel(model_path)\n",
    "    lm.load_model()\n",
    "    prompt = \"Q: What is the largest animal?\\nA:\"\n",
    "    response = lm.generate_response(prompt)\n",
    "    print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c91993c-6b0b-42c2-8dda-ac03a0be5241",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from langchain_core.runnables.base import Runnable\n",
    "\n",
    "class LanguageModel(Runnable):\n",
    "    def __init__(self, model_path: str, device: str = \"auto\", torch_dtype=torch.float16):\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the tokenizer and model.\"\"\"\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path)\n",
    "        self.model = LlamaForCausalLM.from_pretrained(\n",
    "            self.model_path,\n",
    "            torch_dtype=self.torch_dtype,\n",
    "            device_map=self.device,\n",
    "            offload_folder=\"./offload_folder\"  # Add this line\n",
    "        )\n",
    "        print(f\"Model and tokenizer loaded from {self.model_path}\")\n",
    "        return self  # Ensure the object is returned for chaining\n",
    "\n",
    "    def invoke(self, input: dict, config=None) -> str:\n",
    "        \"\"\"\n",
    "        Makes the model runnable by implementing the `invoke` method.\n",
    "        Accepts a dictionary input from LangChain's pipeline.\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            raise ValueError(\"Model and tokenizer must be loaded first using load_model().\")\n",
    "\n",
    "        # Extract the input text from the dictionary\n",
    "        # if \"input\" not in input:\n",
    "        #     raise ValueError(\"Expected a dictionary with an 'input' key.\")\n",
    "        \n",
    "        prompt = input[\"input\"]  # Extract the actual string input\n",
    "    \n",
    "    \n",
    "        # Tokenize input\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "        # Generate output\n",
    "        output_ids = self.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=32\n",
    "        )\n",
    "        \n",
    "        # Decode and return response\n",
    "        response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2e78730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, BitsAndBytesConfig\n",
    "from langchain_core.runnables.base import Runnable\n",
    "import os\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "class LanguageModel(Runnable):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_path: str, \n",
    "        device: str = \"auto\", \n",
    "        torch_dtype: torch.dtype = torch.float16,\n",
    "        max_new_tokens: int = 32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Language Model.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the model or model identifier\n",
    "            device: Device to run the model on (\"auto\", \"cpu\", \"cuda\", etc.)\n",
    "            torch_dtype: Torch data type for model\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.device = device\n",
    "        self.torch_dtype = torch_dtype\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.tokenizer: Optional[LlamaTokenizer] = None\n",
    "        self.model: Optional[LlamaForCausalLM] = None\n",
    "\n",
    "    def load_model(self) -> 'LanguageModel':\n",
    "        \"\"\"\n",
    "        Loads the tokenizer and model.\n",
    "        \n",
    "        Returns:\n",
    "            self for method chaining\n",
    "        \n",
    "        Raises:\n",
    "            OSError: If model loading fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Configure quantization\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                bnb_8bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            # Create offload directory if it doesn't exist\n",
    "            offload_dir = \"./offload_folder\"\n",
    "            os.makedirs(offload_dir, exist_ok=True)\n",
    "\n",
    "            self.tokenizer = LlamaTokenizer.from_pretrained(self.model_path)\n",
    "            self.model = LlamaForCausalLM.from_pretrained(\n",
    "                self.model_path,\n",
    "                torch_dtype=self.torch_dtype,\n",
    "                device_map=\"cpu\",  # Force CPU usage\n",
    "                offload_folder=offload_dir,\n",
    "                quantization_config=quantization_config  # Use the config here\n",
    "            )\n",
    "            print(f\"Model and tokenizer loaded from {self.model_path}\")\n",
    "            return self\n",
    "        except Exception as e:\n",
    "            raise OSError(f\"Failed to load model: {str(e)}\")\n",
    "\n",
    "    def invoke(self, input: Dict[str, Any], config: Optional[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate text based on input prompt.\n",
    "        \n",
    "        Args:\n",
    "            input: Dictionary containing the input prompt\n",
    "            config: Optional configuration dictionary\n",
    "        \n",
    "        Returns:\n",
    "            Generated text response\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If model isn't loaded or input is invalid\n",
    "        \"\"\"\n",
    "        # Validate model is loaded\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            raise ValueError(\"Model and tokenizer must be loaded first using load_model().\")\n",
    "\n",
    "        # Extract and validate input\n",
    "        if not isinstance(input, dict):\n",
    "            raise ValueError(\"Input must be a dictionary\")\n",
    "        \n",
    "        prompt = input.get(\"input\")\n",
    "        if not prompt:\n",
    "            raise ValueError(\"Input dictionary must contain an 'input' key with non-empty value\")\n",
    "\n",
    "        try:\n",
    "            # Move input to same device as model\n",
    "            input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "            if hasattr(self.model, 'device'):\n",
    "                input_ids = input_ids.to(self.model.device)\n",
    "\n",
    "            # Generate with error handling\n",
    "            with torch.no_grad():\n",
    "                output_ids = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    temperature=0.7,  # Add some randomness to generation\n",
    "                    do_sample=True,   # Enable sampling\n",
    "                )\n",
    "\n",
    "            # Decode and return response\n",
    "            response = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during text generation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1057d686-b060-4080-8978-a0b9f4f198c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Failed to load model: name 'BitsAndBytesConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 43\u001b[0m, in \u001b[0;36mLanguageModel.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Configure quantization\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m \u001b[43mBitsAndBytesConfig\u001b[49m(\n\u001b[1;32m     44\u001b[0m         load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     45\u001b[0m         bnb_8bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Create offload directory if it doesn't exist\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BitsAndBytesConfig' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# #model_path = 'openlm-research/open_llama_3b'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model_path = '/Users/aelkhodary/Documents/GitHub/Books/DATA/LLM/openai-generative-ai/openlm-research/open_llama_3b'   # Change to your desired model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# llm = LanguageModel(model_path).load_model()\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize and load the model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLanguageModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenlm-research/open_llama_3b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m---> 10\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate text\u001b[39;00m\n\u001b[1;32m     13\u001b[0m response \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minvoke({\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is artificial intelligence?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m })\n",
      "Cell \u001b[0;32mIn[33], line 62\u001b[0m, in \u001b[0;36mLanguageModel.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Failed to load model: name 'BitsAndBytesConfig' is not defined"
     ]
    }
   ],
   "source": [
    "# #model_path = 'openlm-research/open_llama_3b'\n",
    "# model_path = '/Users/aelkhodary/Documents/GitHub/Books/DATA/LLM/openai-generative-ai/openlm-research/open_llama_3b'   # Change to your desired model\n",
    "# llm = LanguageModel(model_path).load_model()\n",
    "\n",
    "\n",
    "# Initialize and load the model\n",
    "model = LanguageModel(\n",
    "    model_path=\"openlm-research/open_llama_3b\",\n",
    "    max_new_tokens=50\n",
    ").load_model()\n",
    "\n",
    "# Generate text\n",
    "response = model.invoke({\n",
    "    \"input\": \"What is artificial intelligence?\"\n",
    "})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d4127-a00d-47cd-ba5b-fc420c6c7496",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define the model path\n",
    "model_path = 'openlm-research/open_llama_3b'\n",
    "\n",
    "# Load the language model\n",
    "llm = LanguageModel(model_path).load_model()\n",
    "\n",
    "# Define the prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world-class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Define the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create the chain (prompt | llm | output_parser)\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Run the chain\n",
    "output = chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8720d7-3e15-41a7-85f2-eff64b3383e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
